{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd84369c",
   "metadata": {},
   "source": [
    "## 2A. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67be2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b44d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32727 entries, 0 to 32726\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                32727 non-null  object \n",
      " 1   post_title        32727 non-null  object \n",
      " 2   type              32727 non-null  object \n",
      " 3   body              27217 non-null  object \n",
      " 4   score             32727 non-null  int64  \n",
      " 5   url               32727 non-null  object \n",
      " 6   created_utc       32727 non-null  float64\n",
      " 7   text              32727 non-null  object \n",
      " 8   word_count        32727 non-null  int64  \n",
      " 9   char_count        32727 non-null  int64  \n",
      " 10  created_datetime  32727 non-null  object \n",
      " 11  date              32727 non-null  object \n",
      " 12  year              32727 non-null  int64  \n",
      " 13  month             32727 non-null  object \n",
      " 14  day               32727 non-null  int64  \n",
      " 15  dow               32727 non-null  object \n",
      " 16  hour              32727 non-null  int64  \n",
      "dtypes: float64(1), int64(6), object(10)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed data\\sl_all_text_analyzed.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efccd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffdbb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32727/32727 [00:01<00:00, 19658.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1448e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"word_count\", axis=1, inplace=True)\n",
    "df.drop(\"char_count\", axis=1, inplace=True)\n",
    "\n",
    "df[\"word_count\"] = df[\"clean_text\"].apply(lambda x: len(str(x).split()))\n",
    "df[\"char_count\"] = df[\"clean_text\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fdc3f",
   "metadata": {},
   "source": [
    "## Remove posts below and above thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f57537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter dataset size: 32727\n",
      "After filter dataset size: 26187\n",
      "Removed dataset size: 6540\n",
      "Total words in corpus: 2941831\n",
      "Unique words in corpus: 67136\n"
     ]
    }
   ],
   "source": [
    "LOWER_THRESHOLD = 20\n",
    "UPPER_THRESHOLD = 5000\n",
    "\n",
    "df_filtered = df[(df[\"word_count\"] >= LOWER_THRESHOLD) & \n",
    "                 (df[\"word_count\"] <= UPPER_THRESHOLD)].copy()\n",
    "\n",
    "print(\"Before filter dataset size:\", len(df))\n",
    "print(\"After filter dataset size:\", len(df_filtered))\n",
    "print(\"Removed dataset size:\", len(df) - len(df_filtered))\n",
    "\n",
    "\n",
    "all_words = \" \".join(df_filtered[\"clean_text\"]).split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))\n",
    "\n",
    "print(\"Total words in corpus:\", total_words)\n",
    "print(\"Unique words in corpus:\", unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5959655",
   "metadata": {},
   "source": [
    "## 2B. Traditional tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924321ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nisal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4959dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- i. Traditional regex-based tokenization ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def regex_tokenize(text):\n",
    "    # Tokenize using regex (word characters)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase and remove stopwords\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha() and t.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75267136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26187/26187 [00:21<00:00, 1211.64it/s]\n"
     ]
    }
   ],
   "source": [
    "df_filtered[\"tokens_regex\"] = df_filtered[\"clean_text\"].progress_apply(regex_tokenize)\n",
    "\n",
    "# Save tokenized posts\n",
    "df_filtered[[\"tokens_regex\"]].to_pickle(\"models/tokens_regex.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6379bb",
   "metadata": {},
   "source": [
    "## Sub-word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d000bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ii. Sub-word tokenization ---\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import sentencepiece as spm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f170470",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"models/clean_text_corpus.txt\"\n",
    "vocab_size=30000\n",
    "min_frequency=2\n",
    "special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    "\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in df_filtered[\"clean_text\"]:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33cdd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\vocab.json', 'models\\\\merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE Tokenizer\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bpe_tokenizer.train(files=corpus_file, \n",
    "                    vocab_size=vocab_size, \n",
    "                    min_frequency=min_frequency, \n",
    "                    special_tokens=special_tokens)\n",
    "bpe_tokenizer.save_model(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5a5b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\vocab.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPiece Tokenizer (BERT style)\n",
    "wp_tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "wp_tokenizer.train(files=corpus_file, \n",
    "                    vocab_size=vocab_size, \n",
    "                    min_frequency=min_frequency, \n",
    "                    special_tokens=special_tokens)\n",
    "wp_tokenizer.save_model(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ee6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece Tokenizer\n",
    "spm.SentencePieceTrainer.Train(input=corpus_file,\n",
    "                               model_prefix=\"models/sentencepiece\",\n",
    "                               vocab_size=vocab_size,\n",
    "                               character_coverage=1.0,\n",
    "                               model_type='bpe')  # can also use 'unigram' or 'word'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4d167",
   "metadata": {},
   "source": [
    "## 2C. Evaluate tokenization schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c58e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_regex = df_filtered[\"tokens_regex\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b957ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex-based tokenization: Total words = 1526299, Unique words = 54757\n"
     ]
    }
   ],
   "source": [
    "def corpus_stats(token_lists):\n",
    "    all_tokens = [t for tokens in token_lists for t in tokens]\n",
    "    total_words = len(all_tokens)\n",
    "    unique_words = len(set(all_tokens))\n",
    "    return total_words, unique_words\n",
    "\n",
    "# --- Regex-based stats ---\n",
    "total_words_regex, unique_words_regex = corpus_stats(tokens_regex)\n",
    "print(f\"Regex-based tokenization: Total words = {total_words_regex}, Unique words = {unique_words_regex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd61f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = ByteLevelBPETokenizer(\"models/vocab.json\", \"models/merges.txt\")\n",
    "wp_tokenizer = BertWordPieceTokenizer(\"models/vocab.txt\", lowercase=True)\n",
    "sp_model = spm.SentencePieceProcessor(model_file=\"models/sentencepiece.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad48f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26187/26187 [00:09<00:00, 2874.46it/s]\n",
      "100%|██████████| 26187/26187 [00:09<00:00, 2905.89it/s]\n",
      "100%|██████████| 26187/26187 [00:06<00:00, 4113.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenize corpus with sub-word tokenizers ---\n",
    "def tokenize_bpe(text):\n",
    "    return bpe_tokenizer.encode(text).tokens\n",
    "\n",
    "def tokenize_wp(text):\n",
    "    return wp_tokenizer.encode(text).tokens\n",
    "\n",
    "def tokenize_sp(text):\n",
    "    return sp_model.encode(text, out_type=str)\n",
    "\n",
    "# Tokenized lists\n",
    "tokens_bpe = df_filtered[\"clean_text\"].progress_apply(tokenize_bpe).tolist()\n",
    "tokens_wp = df_filtered[\"clean_text\"].progress_apply(tokenize_wp).tolist()\n",
    "tokens_sp = df_filtered[\"clean_text\"].progress_apply(tokenize_sp).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66574160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE: Total words = 3050607, Unique words = 28695\n",
      "WordPiece: Total words = 3084599, Unique words = 28206\n",
      "SentencePiece: Total words = 3049111, Unique words = 28771\n"
     ]
    }
   ],
   "source": [
    "# Stats\n",
    "total_words_bpe, unique_words_bpe = corpus_stats(tokens_bpe)\n",
    "total_words_wp, unique_words_wp = corpus_stats(tokens_wp)\n",
    "total_words_sp, unique_words_sp = corpus_stats(tokens_sp)\n",
    "\n",
    "print(f\"BPE: Total words = {total_words_bpe}, Unique words = {unique_words_bpe}\")\n",
    "print(f\"WordPiece: Total words = {total_words_wp}, Unique words = {unique_words_wp}\")\n",
    "print(f\"SentencePiece: Total words = {total_words_sp}, Unique words = {unique_words_sp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1440687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probs(token_lists):\n",
    "    all_tokens = [t for tokens in token_lists for t in tokens]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    total = sum(token_counts.values())\n",
    "    probs = {tok: count/total for tok, count in token_counts.items()}\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca268c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(token_lists, probs):\n",
    "    N = sum(len(tokens) for tokens in token_lists)\n",
    "    log_prob_sum = 0\n",
    "    for tokens in token_lists:\n",
    "        for t in tokens:\n",
    "            p = probs.get(t, 1e-8)  # smoothing\n",
    "            log_prob_sum += math.log(p)\n",
    "    ppl = math.exp(-log_prob_sum / N)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5311fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex: 3664.22\n",
      "BPE: 1394.05\n",
      "SentencePiece: 1352.82\n",
      "WordPiece: 1327.60\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity for each scheme\n",
    "ppl_regex = perplexity(tokens_regex, unigram_probs(tokens_regex))\n",
    "ppl_bpe = perplexity(tokens_bpe, unigram_probs(tokens_bpe))\n",
    "ppl_wp = perplexity(tokens_wp, unigram_probs(tokens_wp))\n",
    "ppl_sp = perplexity(tokens_sp, unigram_probs(tokens_sp))\n",
    "\n",
    "perplexities = {\n",
    "    \"Regex\": ppl_regex,\n",
    "    \"BPE\": ppl_bpe,\n",
    "    \"WordPiece\": ppl_wp,\n",
    "    \"SentencePiece\": ppl_sp\n",
    "}\n",
    "\n",
    "for name, val in sorted(perplexities.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde3919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop(['post_title', 'body', 'score','url','text','created_datetime','date','year','month','day','dow','hour','tokens_regex'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df02229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26187 entries, 0 to 32726\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           26187 non-null  object \n",
      " 1   type         26187 non-null  object \n",
      " 2   created_utc  26187 non-null  float64\n",
      " 3   clean_text   26187 non-null  object \n",
      " 4   word_count   26187 non-null  int64  \n",
      " 5   char_count   26187 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e0488e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\"cleaned data\\sl_all_text_cleaned.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
