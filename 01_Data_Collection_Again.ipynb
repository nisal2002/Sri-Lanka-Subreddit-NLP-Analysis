{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8467ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97570c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"y25BJERt0d-YtQKuTk7gUQ\",\n",
    "    client_secret=\"fBb7prfE4DcIj2Cv4l7Dw73ziLZFyQ\",\n",
    "    user_agent=\"sl-nlp-project:v1 (by /u/SpareUse7114)\", \n",
    ")\n",
    "reddit.read_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8663665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subreddit(display_name='srilanka')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit = reddit.subreddit(\"srilanka\")\n",
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b64b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=[\n",
    "  \"A/l\", \"Abuse\", \"Admission\", \"Alevel\", \"Alumni\", \"Amantha\", \"Amaradeva\", \"Another\", \"Any\", \"Arjuna\",\n",
    "  \"Assignment\", \"Bolt\", \"Boy\", \"Campus\", \"Cardinal\", \"Cars\", \"Certificate\", \"Charges\", \"College\", \"Coursework\",\n",
    "  \"Curriculum\", \"Day\", \"Degree\", \"Department\", \"Dhanith\", \"Dhoni\", \"Dilshan\", \"Dimuth\", \"Diploma\", \"Drugs\",\n",
    "  \"Education system\", \"Enrollment\", \"Every\", \"Exam\", \"Faculty\", \"Ganja\",\"Marijuana\", \"Genz\", \"Get\", \"Girl\", \"Graduation\",\n",
    "  \"Guy\", \"Hamilton\", \"Harini\", \"High\", \"Homework\", \"Ice\", \"Imran\", \"Institute\", \"Iraj\", \"Ishara\",\n",
    "  \"Jacqueline\", \"Jerome\", \"Johnston\", \"Jothipala\", \"Kanjipaana\", \"Kindergarden\", \"Know\", \"Kohli\", \"Kosgoda\", \"Kush\",\n",
    "  \"Last\", \"Learning\", \"Lecturer\", \"Life\", \"Madush\", \"Mahela\", \"Makandure\", \"Malcolm\", \"Malinga\",\"Mom\",\"Nvidia\",\"Rysen\"\n",
    "  \"Man\", \"Mathews\", \"Messi\", \"Montessori\", \"Month\", \"Murali\", \"Murder\", \"New\", \"Neymar\", \"Now\",\"Dad\",\n",
    "  \"O/l\", \"Offences\", \"Olevel\", \"Only\", \"Other\", \"Pastor\", \"People\", \"Phelps\", \"Preschool\", \"Primary\",\n",
    "  \"Professor\", \"Project\", \"Ranjan\", \"Ranjith\", \"Really\", \"Rohit\", \"Ronaldo\", \"Sachin\", \"Sanga\",\n",
    "  \"Scholarship\", \"Secondary\", \"See\", \"Sewwandi\", \"Sex\", \"Situationship\", \"Some\", \"Study\", \"Sunil perera\", \"Susanthika\",\n",
    "  \"Take\", \"Test\", \"Tharaka\", \"Thesis\", \"Thing\", \"Time\", \"Today\", \"Tomorrow\", \"Trafficking\", \"Tuition\",\n",
    "  \"Underworld\", \"Very\", \"Virat\", \"Way\", \"Week\", \"Woman\", \"Year\", \"Yesterday\", \"Yohani\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4917307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 keywords to search\n"
     ]
    }
   ],
   "source": [
    "print(len(keywords), \"keywords to search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b0bba",
   "metadata": {},
   "source": [
    "## Text Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f48d6f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [02:01,  1.88it/s]\n",
      "228it [02:00,  1.89it/s]\n",
      "103it [00:54,  1.89it/s]\n",
      "56it [00:29,  1.90it/s]\n",
      "26it [00:13,  1.88it/s]\n",
      "5it [00:03,  1.65it/s]\n",
      "9it [00:05,  1.77it/s]\n",
      "222it [01:57,  1.88it/s]\n",
      "232it [02:01,  1.91it/s]\n",
      "20it [00:10,  1.84it/s]\n",
      "164it [01:26,  1.90it/s]\n",
      "11it [00:06,  1.81it/s]\n",
      "222it [01:57,  1.90it/s]\n",
      "199it [01:43,  1.92it/s]\n",
      "27it [00:14,  1.86it/s]\n",
      "241it [02:06,  1.90it/s]\n",
      "233it [02:02,  1.90it/s]\n",
      "234it [02:02,  1.90it/s]\n",
      "233it [02:02,  1.90it/s]\n",
      "17it [00:09,  1.83it/s]\n",
      "89it [00:46,  1.91it/s]\n",
      "234it [02:03,  1.90it/s]\n",
      "239it [02:05,  1.91it/s]\n",
      "235it [02:04,  1.89it/s]\n",
      "7it [00:04,  1.72it/s]\n",
      "6it [00:03,  1.66it/s]\n",
      "38it [00:20,  1.87it/s]\n",
      "16it [00:08,  1.82it/s]\n",
      "224it [01:57,  1.91it/s]\n",
      "222it [01:57,  1.89it/s]\n",
      "186it [01:38,  1.88it/s]\n",
      "163it [01:26,  1.89it/s]\n",
      "224it [01:59,  1.88it/s]\n",
      "226it [01:59,  1.90it/s]\n",
      "207it [01:48,  1.90it/s]\n",
      "27it [00:14,  1.86it/s]\n",
      "19it [00:10,  1.84it/s]\n",
      "12it [00:06,  1.79it/s]\n",
      "230it [02:01,  1.89it/s]\n",
      "210it [01:50,  1.90it/s]\n",
      "232it [02:02,  1.89it/s]\n",
      "220it [01:55,  1.90it/s]\n",
      "5it [00:03,  1.67it/s]\n",
      "34it [00:18,  1.87it/s]\n",
      "221it [01:57,  1.88it/s]\n",
      "23it [00:12,  1.84it/s]\n",
      "138it [01:13,  1.89it/s]\n",
      "15it [00:08,  1.80it/s]\n",
      "231it [02:02,  1.88it/s]\n",
      "49it [00:25,  1.89it/s]\n",
      "8it [00:04,  1.76it/s]\n",
      "3it [00:01,  1.51it/s]\n",
      "35it [00:18,  1.87it/s]\n",
      "13it [00:07,  1.78it/s]\n",
      "4it [00:02,  1.61it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "236it [02:04,  1.90it/s]\n",
      "15it [00:08,  1.80it/s]\n",
      "12it [00:06,  1.80it/s]\n",
      "4it [00:02,  1.58it/s]\n",
      "229it [02:01,  1.89it/s]\n",
      "221it [01:55,  1.91it/s]\n",
      "230it [02:01,  1.89it/s]\n",
      "215it [01:53,  1.89it/s]\n",
      "1it [00:00,  1.05it/s]\n",
      "35it [00:18,  1.86it/s]\n",
      "1it [00:00,  1.07it/s]\n",
      "12it [00:06,  1.75it/s]\n",
      "55it [00:29,  1.88it/s]\n",
      "218it [01:55,  1.89it/s]\n",
      "37it [00:20,  1.84it/s]\n",
      "0it [00:00, ?it/s]\n",
      "32it [00:17,  1.85it/s]\n",
      "41it [00:21,  1.88it/s]\n",
      "2it [00:01,  1.38it/s]\n",
      "232it [02:03,  1.88it/s]\n",
      "26it [00:14,  1.86it/s]\n",
      "202it [01:46,  1.89it/s]\n",
      "237it [02:05,  1.89it/s]\n",
      "1it [00:00,  1.06it/s]\n",
      "229it [02:00,  1.90it/s]\n",
      "225it [01:58,  1.90it/s]\n",
      "227it [01:59,  1.90it/s]\n",
      "56it [00:29,  1.88it/s]\n",
      "15it [00:08,  1.84it/s]\n",
      "222it [01:57,  1.89it/s]\n",
      "229it [02:00,  1.90it/s]\n",
      "15it [00:08,  1.81it/s]\n",
      "226it [01:59,  1.89it/s]\n",
      "0it [00:00, ?it/s]\n",
      "10it [00:05,  1.79it/s]\n",
      "124it [01:05,  1.88it/s]\n",
      "71it [00:37,  1.90it/s]\n",
      "228it [02:01,  1.88it/s]\n",
      "44it [00:23,  1.89it/s]\n",
      "28it [00:15,  1.86it/s]\n",
      "230it [02:01,  1.90it/s]\n",
      "6it [00:03,  1.68it/s]\n",
      "8it [00:05,  1.59it/s]\n",
      "4it [00:02,  1.58it/s]\n",
      "22it [00:11,  1.84it/s]\n",
      "195it [01:42,  1.91it/s]\n",
      "54it [00:28,  1.89it/s]\n",
      "217it [01:54,  1.89it/s]\n",
      "1it [00:00,  1.11it/s]\n",
      "208it [01:50,  1.89it/s]\n",
      "8it [00:04,  1.73it/s]\n",
      "232it [02:02,  1.89it/s]\n",
      "224it [01:57,  1.90it/s]\n",
      "9it [00:05,  1.76it/s]\n",
      "4it [00:02,  1.59it/s]\n",
      "235it [02:03,  1.90it/s]\n",
      "231it [02:01,  1.89it/s]\n",
      "6it [00:03,  1.70it/s]\n",
      "15it [00:08,  1.81it/s]\n",
      "227it [01:59,  1.90it/s]\n",
      "237it [02:15,  1.74it/s]\n",
      "225it [01:59,  1.89it/s]\n",
      "229it [02:09,  1.77it/s]\n",
      "26it [00:14,  1.83it/s]\n",
      "191it [01:40,  1.89it/s]\n",
      "19it [00:10,  1.75it/s]\n",
      "229it [02:00,  1.90it/s]\n",
      "7it [00:04,  1.74it/s]\n",
      "232it [02:01,  1.91it/s]\n",
      "234it [02:02,  1.90it/s]\n",
      "216it [01:54,  1.89it/s]\n",
      "230it [02:01,  1.89it/s]\n",
      "232it [02:02,  1.89it/s]\n",
      "30it [00:16,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "limit_per_keyword = 1000  \n",
    "posts_data = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    for submission in tqdm(subreddit.search(keyword, limit=limit_per_keyword)):\n",
    "        try:\n",
    "            if submission.title.strip() == \"\" and submission.selftext.strip() == \"\":\n",
    "                continue  \n",
    "            \n",
    "            posts_data.append({\n",
    "                    \"id\": submission.id,\n",
    "                    \"post_title\": submission.title,\n",
    "                    \"type\": \"text\",\n",
    "                    \"body\": submission.selftext,\n",
    "                    \"score\": submission.score,\n",
    "                    \"url\": submission.url,\n",
    "                    \"created_utc\": submission.created_utc\n",
    "            })\n",
    "            \n",
    "            time.sleep(0.5)  # respect Reddit API rate limit\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e554810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>type</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1olnojq</td>\n",
       "      <td>A/L Maths vs Bio struggle</td>\n",
       "      <td>text</td>\n",
       "      <td>I did biology for A/L in my first shy(2024) an...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/srilanka/comments/1ol...</td>\n",
       "      <td>1.762003e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13i3pwh</td>\n",
       "      <td>Is A/L really that much hard?</td>\n",
       "      <td>text</td>\n",
       "      <td>[%20Text%20]</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.reddit.com/r/srilanka/comments/13i...</td>\n",
       "      <td>1.684145e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1o10ltt</td>\n",
       "      <td>An average amount of books for A/Ls</td>\n",
       "      <td>text</td>\n",
       "      <td>Was cleaning out the house and took a pic of m...</td>\n",
       "      <td>103</td>\n",
       "      <td>https://i.redd.it/mcc1pbcpbttf1.png</td>\n",
       "      <td>1.759896e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1mamxjv</td>\n",
       "      <td>If the Sri Lankan A/L system is designed to se...</td>\n",
       "      <td>text</td>\n",
       "      <td>I'm genuinely curious about this. The GCE A/L ...</td>\n",
       "      <td>44</td>\n",
       "      <td>https://www.reddit.com/r/srilanka/comments/1ma...</td>\n",
       "      <td>1.753625e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ml1b08</td>\n",
       "      <td>Exceptional student’s A/L results shockingly l...</td>\n",
       "      <td>text</td>\n",
       "      <td>Hi everyone,\\n\\nI’m posting on behalf of a ver...</td>\n",
       "      <td>66</td>\n",
       "      <td>https://www.reddit.com/r/srilanka/comments/1ml...</td>\n",
       "      <td>1.754674e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                         post_title  type  \\\n",
       "0  1olnojq                          A/L Maths vs Bio struggle  text   \n",
       "1  13i3pwh                      Is A/L really that much hard?  text   \n",
       "2  1o10ltt                An average amount of books for A/Ls  text   \n",
       "3  1mamxjv  If the Sri Lankan A/L system is designed to se...  text   \n",
       "4  1ml1b08  Exceptional student’s A/L results shockingly l...  text   \n",
       "\n",
       "                                                body  score  \\\n",
       "0  I did biology for A/L in my first shy(2024) an...      5   \n",
       "1                                       [%20Text%20]     10   \n",
       "2  Was cleaning out the house and took a pic of m...    103   \n",
       "3  I'm genuinely curious about this. The GCE A/L ...     44   \n",
       "4  Hi everyone,\\n\\nI’m posting on behalf of a ver...     66   \n",
       "\n",
       "                                                 url   created_utc  \n",
       "0  https://www.reddit.com/r/srilanka/comments/1ol...  1.762003e+09  \n",
       "1  https://www.reddit.com/r/srilanka/comments/13i...  1.684145e+09  \n",
       "2                https://i.redd.it/mcc1pbcpbttf1.png  1.759896e+09  \n",
       "3  https://www.reddit.com/r/srilanka/comments/1ma...  1.753625e+09  \n",
       "4  https://www.reddit.com/r/srilanka/comments/1ml...  1.754674e+09  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_posts = pd.DataFrame(posts_data)\n",
    "df_text_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1253f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15472 entries, 0 to 15471\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           15472 non-null  object \n",
      " 1   post_title   15472 non-null  object \n",
      " 2   type         15472 non-null  object \n",
      " 3   body         15472 non-null  object \n",
      " 4   score        15472 non-null  int64  \n",
      " 5   url          15472 non-null  object \n",
      " 6   created_utc  15472 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 846.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_text_posts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e9ba1",
   "metadata": {},
   "source": [
    "## Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5975d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"raw data\\srilanka_posts_text.csv\")\n",
    "df2 = pd.read_csv(\"raw data\\srilanka_posts_non_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f90489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df1, df2, df_text_posts], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b75bce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73029 entries, 0 to 73028\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           73029 non-null  object \n",
      " 1   post_title   73029 non-null  object \n",
      " 2   type         73029 non-null  object \n",
      " 3   body         65069 non-null  object \n",
      " 4   score        73029 non-null  int64  \n",
      " 5   url          73029 non-null  object \n",
      " 6   created_utc  73029 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c191b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to raw data\\srilanka_all_text.csv\n"
     ]
    }
   ],
   "source": [
    "if not combined_df.empty:\n",
    "    combined_df.to_csv(\"raw data\\srilanka_all_text.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved to raw data\\srilanka_all_text.csv\")\n",
    "else:\n",
    "    print(\"\\nNo data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
